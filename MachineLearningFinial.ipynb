{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd24985",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "if os.path.exists('/mnt/data/heart_cleaned.csv'):\n",
    "    df = pd.read_csv('/mnt/data/heart_cleaned.csv')\n",
    "elif os.path.exists('heart_cleaned.csv'):\n",
    "    df = pd.read_csv('heart_cleaned.csv')\n",
    "else:\n",
    "    df = pd.read_csv('heart.csv')\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee9038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['age_group'] = pd.cut(\n",
    "    df['age'], bins=[0, 35, 45, 55, 65, 100],\n",
    "    labels=['<35', '35-44', '45-54', '55-64', '65+']\n",
    ")\n",
    "\n",
    "if 'chol' in df.columns:\n",
    "    df['chol_per_age'] = df['chol'] / df['age']\n",
    "\n",
    "if 'trestbps' in df.columns and 'chol' in df.columns:\n",
    "    df['bp_chol_ratio'] = df['trestbps'] / df['chol']\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object' or str(df[col].dtype) == 'category':\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    else:\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "for col in cat_cols:\n",
    "    if df[col].nunique() <= 2:\n",
    "        df[col] = LabelEncoder().fit_transform(df[col])\n",
    "    else:\n",
    "        df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "df[num_cols] = StandardScaler().fit_transform(df[num_cols])\n",
    "df[num_cols] = MinMaxScaler().fit_transform(df[num_cols])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e25551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df.iloc[:, [x for x in range(0,16)]],\n",
    "    df.iloc[:, [x for x in range(16,20)]],\n",
    "    train_size=0.7, test_size=0.15, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "x_validate, y_validate = (\n",
    "    df.iloc[[x for x in range(871, len(df))], [x for x in range(0,16)]],\n",
    "    df.iloc[[x for x in range(871, len(df))], [x for x in range(16,20)]],\n",
    ")\n",
    "\n",
    "def to_flat(arr2d, col_idx):\n",
    "    return np.array(arr2d.iloc[:, [col_idx]]).ravel()\n",
    "\n",
    "y_train_groups = [to_flat(y_train, i) for i in range(y_train.shape[1])]\n",
    "y_test_groups = [to_flat(y_test, i) for i in range(y_test.shape[1])]\n",
    "y_validate_groups = [to_flat(y_validate, i) for i in range(y_validate.shape[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace04476",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_constructors = {\n",
    "    'Logistic': lambda: LogisticRegression(max_iter=1000),\n",
    "    'DecisionTree': lambda: DecisionTreeClassifier(max_depth=200, max_leaf_nodes=50),\n",
    "    'RandomForest': lambda: RandomForestClassifier(max_depth=50, max_leaf_nodes=25, n_estimators=100),\n",
    "    'AdaBoost': lambda: AdaBoostClassifier(n_estimators=100, learning_rate=0.1),\n",
    "    'KNN': lambda: KNeighborsClassifier(n_neighbors=10, n_jobs=2),\n",
    "    'SVC': lambda: SVC(probability=True)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def metrics_from_binary(y_true, y_pred, y_prob=None):\n",
    "    res = {}\n",
    "    res['accuracy'] = round(accuracy_score(y_true, y_pred), 4)\n",
    "    res['precision'] = round(precision_score(y_true, y_pred, zero_division=0), 4)\n",
    "    res['recall'] = round(recall_score(y_true, y_pred), 4)\n",
    "    res['f1'] = round(f1_score(y_true, y_pred), 4)\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            res['roc_auc'] = round(roc_auc_score(y_true, y_prob), 4)\n",
    "        except:\n",
    "            res['roc_auc'] = None\n",
    "    else:\n",
    "        res['roc_auc'] = None\n",
    "    return res\n",
    "\n",
    "def safe_probs(clf, X):\n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        return clf.predict_proba(X)[:, 1]\n",
    "    elif hasattr(clf, 'decision_function'):\n",
    "        df = clf.decision_function(X)\n",
    "        return 1 / (1 + np.exp(-df))\n",
    "    else:\n",
    "        return clf.predict(X).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf7f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_summary = {}\n",
    "\n",
    "for grp_idx in range(4):\n",
    "\n",
    "    print(\"\\n------------------------------------------------------------\")\n",
    "    print(f'Age-group target index: {grp_idx}')\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "    ytr = y_train_groups[grp_idx]\n",
    "    yval = y_validate_groups[grp_idx]\n",
    "    yte = y_test_groups[grp_idx]\n",
    "\n",
    "    trained = {}\n",
    "    val_probs = {}\n",
    "    test_probs = {}\n",
    "    val_metrics = {}\n",
    "\n",
    "    for name, ctor in model_constructors.items():\n",
    "        clf = ctor()\n",
    "        try:\n",
    "            clf.fit(x_train, ytr)\n",
    "        except Exception as e:\n",
    "            print(f'WARNING: {name} failed:', e)\n",
    "            continue\n",
    "\n",
    "        trained[name] = clf\n",
    "        val_probs[name] = safe_probs(clf, x_validate)\n",
    "        test_probs[name] = safe_probs(clf, x_test)\n",
    "\n",
    "        val_pred_label = (val_probs[name] >= 0.5).astype(int)\n",
    "        val_metrics[name] = metrics_from_binary(yval, val_pred_label, val_probs[name])\n",
    "\n",
    "    def auc_or_neg(name):\n",
    "        auc = val_metrics[name]['roc_auc']\n",
    "        return auc if auc is not None else -1\n",
    "\n",
    "    top3 = sorted(trained.keys(), key=auc_or_neg, reverse=True)[:3]\n",
    "    print('Top-3 models:', top3)\n",
    "\n",
    "    avg_val_prob = np.mean([val_probs[m] for m in top3], axis=0)\n",
    "    avg_test_prob = np.mean([test_probs[m] for m in top3], axis=0)\n",
    "\n",
    "    vote_estimators = [(m, model_constructors[m]()) for m in top3]\n",
    "    try:\n",
    "        vote_clf = VotingClassifier(vote_estimators, voting='soft')\n",
    "        vote_clf.fit(x_train, ytr)\n",
    "        vote_val_prob = vote_clf.predict_proba(x_validate)[:, 1]\n",
    "        vote_test_prob = vote_clf.predict_proba(x_test)[:, 1]\n",
    "    except:\n",
    "        vote_val_prob = avg_val_prob\n",
    "        vote_test_prob = avg_test_prob\n",
    "\n",
    "    eps = 1e-12\n",
    "    loglikes = {}\n",
    "\n",
    "    for m in top3:\n",
    "        p = np.clip(val_probs[m], eps, 1-eps)\n",
    "        ll = np.sum(yval*np.log(p) + (1-yval)*np.log(1-p))\n",
    "        loglikes[m] = ll\n",
    "\n",
    "    ll_vals = np.array(list(loglikes.values()))\n",
    "    centered = ll_vals - ll_vals.max()\n",
    "    w_unnorm = np.exp(centered)\n",
    "    weights = w_unnorm / w_unnorm.sum()\n",
    "\n",
    "    bma_val_prob, bma_test_prob = 0, 0\n",
    "    for w, m in zip(weights, top3):\n",
    "        bma_val_prob += w * val_probs[m]\n",
    "        bma_test_prob += w * test_probs[m]\n",
    "\n",
    "    table = []\n",
    "\n",
    "    def add_row(name, vp, tp):\n",
    "        return {\n",
    "            'model': name,\n",
    "            'val_accuracy': metrics_from_binary(yval, (vp>=0.5).astype(int), vp)['accuracy'],\n",
    "            'val_precision': metrics_from_binary(yval, (vp>=0.5).astype(int), vp)['precision'],\n",
    "            'val_recall': metrics_from_binary(yval, (vp>=0.5).astype(int), vp)['recall'],\n",
    "            'val_f1': metrics_from_binary(yval, (vp>=0.5).astype(int), vp)['f1'],\n",
    "            'val_roc_auc': metrics_from_binary(yval, (vp>=0.5).astype(int), vp)['roc_auc'],\n",
    "            'test_accuracy': metrics_from_binary(yte, (tp>=0.5).astype(int), tp)['accuracy'],\n",
    "            'test_precision': metrics_from_binary(yte, (tp>=0.5).astype(int), tp)['precision'],\n",
    "            'test_recall': metrics_from_binary(yte, (tp>=0.5).astype(int), tp)['recall'],\n",
    "            'test_f1': metrics_from_binary(yte, (tp>=0.5).astype(int), tp)['f1'],\n",
    "            'test_roc_auc': metrics_from_binary(yte, (tp>=0.5).astype(int), tp)['roc_auc'],\n",
    "        }\n",
    "\n",
    "    for m in top3:\n",
    "        table.append(add_row(m, val_probs[m], test_probs[m]))\n",
    "\n",
    "    table.append(add_row('Average_top3', avg_val_prob, avg_test_prob))\n",
    "    table.append(add_row('Voting_top3', vote_val_prob, vote_test_prob))\n",
    "    table.append(add_row('BMA_top3', bma_val_prob, bma_test_prob))\n",
    "\n",
    "    results_summary[f'group_{grp_idx}'] = {\n",
    "        'top3': top3,\n",
    "        'bma_weights': dict(zip(top3, weights)),\n",
    "        'eval': pd.DataFrame(table).set_index('model')\n",
    "    }\n",
    "\n",
    "    display(results_summary[f'group_{grp_idx}']['eval'])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
